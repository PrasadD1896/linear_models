{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668f0e76",
   "metadata": {},
   "source": [
    "# Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcac3e0",
   "metadata": {},
   "source": [
    "## Creator : Prasad Deshmukh\n",
    "## LinkedIn : https://www.linkedin.com/in/prasad-deshmukh-b55b51221\n",
    "## Github : https://github.com/PrasadD1896\n",
    "## Medium :  https://medium.com/@prasaddeshmukh1896"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef7e3c",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c6757",
   "metadata": {},
   "source": [
    "Linear regression is a type of statistical modeling technique that is used to estimate the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as the predictor variables). The goal of linear regression is to find the best-fitting line or curve that describes the relationship between the variables.\n",
    "\n",
    "In simple linear regression, there is only one independent variable and one dependent variable, and the relationship between them is assumed to be linear. The equation for a simple linear regression model is y = mx + b, where y is the dependent variable, x is the independent variable, m is the slope of the line, and b is the y-intercept.\n",
    "\n",
    "In multiple linear regression, there are multiple independent variables and one dependent variable, and the relationship between them is still assumed to be linear. The equation for a multiple linear regression model is y = b0 + b1x1 + b2x2 + ... + bnxn, where y is the dependent variable, x1, x2, ..., xn are the independent variables, and b0, b1, b2, ..., bn are the coefficients that describe the relationship between the variables.\n",
    "\n",
    "Linear regression models are commonly used in many fields, including economics, finance, social sciences, and engineering, among others. They can be used to make predictions, identify relationships between variables, and test hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d80adf",
   "metadata": {},
   "source": [
    "## Applications of Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c33fe7",
   "metadata": {},
   "source": [
    "Linear regression has numerous applications across a variety of fields, and some common examples include:\n",
    "\n",
    "1. Predictive analysis: Linear regression can be used to predict future values of a dependent variable based on the values of independent variables. For example, a company might use linear regression to predict future sales based on factors such as advertising spending, pricing, and seasonality.\n",
    "\n",
    "2. Risk analysis: Linear regression can be used to assess the relationship between risk factors and outcomes. For example, a   financial institution might use linear regression to predict the risk of default based on factors such as credit score, debt-to-income ratio, and loan amount.\n",
    "\n",
    "3. Quality control: Linear regression can be used to identify factors that affect the quality of a product or process. For example, a manufacturer might use linear regression to identify factors that affect the strength of a material, such as temperature and pressure.\n",
    "\n",
    "4. Marketing research: Linear regression can be used to analyze the relationship between marketing efforts and customer behavior. For example, a retailer might use linear regression to analyze the relationship between advertising spending and sales.\n",
    "\n",
    "5. Environmental science: Linear regression can be used to analyze the relationship between environmental factors and outcomes. For example, a scientist might use linear regression to analyze the relationship between temperature and the growth of a particular species.\n",
    "\n",
    "Overall, linear regression is a powerful tool for analyzing relationships between variables and making predictions. Its versatility and broad range of applications make it a valuable tool in many different fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e1708e",
   "metadata": {},
   "source": [
    "## Different Types of Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73204b05",
   "metadata": {},
   "source": [
    "There are several types of linear regression models, some of the most common ones are:\n",
    "\n",
    "1. Simple Linear Regression: This is the simplest type of linear regression model, which involves only one independent variable and one dependent variable. It is used to model the linear relationship between the two variables.\n",
    "\n",
    "2. Multiple Linear Regression: This model involves more than one independent variable and one dependent variable. It is used to model the linear relationship between the dependent variable and multiple independent variables.\n",
    "\n",
    "3. Polynomial Regression: This model involves a nonlinear relationship between the dependent variable and independent variable(s). It is used when the relationship between the variables cannot be adequately described by a straight line.\n",
    "\n",
    "4. Ridge Regression: This model is a type of linear regression that is used when there is multicollinearity (high correlation) between the independent variables. It adds a penalty term to the regression equation to prevent overfitting.\n",
    "\n",
    "5. Lasso Regression: This model is similar to ridge regression, but it uses a different penalty term that results in some of the coefficients being set to zero. This can be useful for variable selection in situations where there are many independent variables.\n",
    "\n",
    "6. Elastic Net Regression: This model is a combination of ridge and lasso regression, and it is used to handle situations where there are many independent variables with multicollinearity.\n",
    "\n",
    "7. Logistic Regression: This model is used to model the relationship between a binary dependent variable and one or more independent variables. Unlike other types of linear regression, it is used for classification rather than prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a951817",
   "metadata": {},
   "source": [
    "Let's understand all one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a6b4f",
   "metadata": {},
   "source": [
    "## 1. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cff70",
   "metadata": {},
   "source": [
    "Simple linear regression is a statistical method that allows you to investigate the relationship between two continuous variables, usually denoted as X and Y. The goal of simple linear regression is to find a linear relationship between these two variables, such that changes in X can be used to predict changes in Y.\n",
    "\n",
    "The model takes the form of Y = β0 + β1X + ε, where Y is the dependent variable, X is the independent variable, β0 is the intercept, β1 is the regression coefficient (also known as the slope), and ε is the error term. The error term represents the part of the variation in Y that cannot be explained by the linear relationship with X.\n",
    "\n",
    "To estimate the values of β0 and β1, we can use a method called least squares regression. This involves finding the line that minimizes the sum of the squared differences between the predicted values of Y and the actual values of Y.\n",
    "\n",
    "Once we have estimated the values of β0 and β1, we can use the equation to make predictions about the value of Y based on the value of X. We can also use statistical tests to assess the significance of the relationship between X and Y, as well as the accuracy of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e132a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Let's create some sample data\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 4, 9, 16, 25, 36, 49, 64, 81])\n",
    "\n",
    "# We need to reshape the data to fit the model\n",
    "x = x.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Let's create a linear regression object\n",
    "model = LinearRegression()\n",
    "\n",
    "# Let's fit the model to the data\n",
    "model.fit(x, y)\n",
    "\n",
    "# Let's try making predictions using the model\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# Let's plot the data and the regression line\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred, color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef959f9",
   "metadata": {},
   "source": [
    "## 2. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892abf2",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical method that extends the idea of simple linear regression to include multiple independent variables. The goal of multiple linear regression is to find a linear relationship between a dependent variable (Y) and two or more independent variables (X1, X2, X3, etc.), such that changes in these independent variables can be used to predict changes in the dependent variable.\n",
    "\n",
    "The model takes the form of Y = β0 + β1X1 + β2X2 + β3X3 + ... + ε, where Y is the dependent variable, X1, X2, X3, etc. are the independent variables, β0 is the intercept, β1, β2, β3, etc. are the regression coefficients (also known as the slopes), and ε is the error term. The error term represents the part of the variation in Y that cannot be explained by the linear relationship with X1, X2, X3, etc.\n",
    "\n",
    "To estimate the values of β0, β1, β2, β3, etc., we can use a method called least squares regression. This involves finding the line that minimizes the sum of the squared differences between the predicted values of Y and the actual values of Y.\n",
    "\n",
    "Once you have estimated the values of β0, β1, β2, β3, etc., we can use the equation to make predictions about the value of Y based on the values of X1, X2, X3, etc. We can also use statistical tests to assess the significance of the relationship between the independent variables and the dependent variable, as well as the accuracy of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309d0c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3333333333333335 [0.11111111 0.11111111 0.11111111]\n"
     ]
    }
   ],
   "source": [
    "# Let's import required libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Let's Generate some sample data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([4, 5, 6])\n",
    "\n",
    "# Let's Create the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print the model coefficients\n",
    "print(model.intercept_, model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78143ab",
   "metadata": {},
   "source": [
    "In this example, the independent variable matrix X has 3 columns (X1, X2, X3) and 3 rows of data. The dependent variable y is a vector of length 3.\n",
    "\n",
    "The LinearRegression() function creates a new linear regression model object, and the fit() method fits the model to the data. The intercept_ attribute contains the intercept (β0), and the coef_ attribute contains the regression coefficients (β1, β2, β3, etc.).\n",
    "\n",
    "Once the model is fitted, you can use the predict() method to make predictions for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd5a23",
   "metadata": {},
   "source": [
    "## 3. Polynomial Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739a968",
   "metadata": {},
   "source": [
    "Polynomial linear regression is a type of linear regression in which the relationship between the independent variable X and dependent variable Y is modeled as an nth degree polynomial. It is useful when the relationship between X and Y is not linear, but can be approximated by a polynomial function.\n",
    "\n",
    "The polynomial regression model takes the form of:\n",
    "\n",
    "Y = β0 + β1X + β2X^2 + β3X^3 + ... + βnX^n + ε\n",
    "\n",
    "where Y is the dependent variable, X is the independent variable, β0 is the intercept, β1, β2, β3, ..., βn are the regression coefficients, X^2, X^3, ..., X^n are the higher-order terms of X, and ε is the error term.\n",
    "\n",
    "To estimate the values of β0, β1, β2, β3, ..., βn, we can use a method called least squares regression. This involves finding the polynomial function that minimizes the sum of the squared differences between the predicted values of Y and the actual values of Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa1b01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000053 [0.         2.54285714 0.14285714]\n"
     ]
    }
   ],
   "source": [
    "# Let's import required libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Let's Generate some sample data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape((-1, 1))\n",
    "y = np.array([3, 8, 10, 12, 18])\n",
    "\n",
    "# Let's Create polynomial features of degree 2\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Let's Create the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Print the model coefficients\n",
    "print(model.intercept_, model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913dc803",
   "metadata": {},
   "source": [
    "## 4. Ridge Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7564d99",
   "metadata": {},
   "source": [
    "Ridge regression is a type of linear regression that is used to handle multicollinearity (i.e., when two or more independent variables are highly correlated with each other) in the dataset. Ridge regression adds a penalty term to the sum of squared errors in the linear regression equation to shrink the regression coefficients towards zero, thereby reducing the variance of the estimates.\n",
    "\n",
    "The ridge regression equation takes the following form:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + β3X3 + ... + βnXn + ε\n",
    "where Y is the dependent variable, X1, X2, X3, ..., Xn are the independent variables, β0, β1, β2, β3, ..., βn are the regression coefficients, and ε is the error term.\n",
    "\n",
    "To perform ridge regression, we need to determine the value of the regularization parameter (λ) that balances the trade-off between reducing the variance of the estimates and increasing the bias. We can use cross-validation to find the optimal value of λ that minimizes the mean squared error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fecd969b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3486238532110106 [0.11009174 0.11009174 0.11009174]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Generate some sample data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([4, 5, 6])\n",
    "\n",
    "# Create the Ridge regression model\n",
    "model = Ridge(alpha=0.5)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print the model coefficients\n",
    "print(model.intercept_, model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5e308",
   "metadata": {},
   "source": [
    "## 5. Lasso Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195342f0",
   "metadata": {},
   "source": [
    "Lasso regression is another type of linear regression that is used to handle multicollinearity in the data. Like Ridge regression, it adds a regularization term to the cost function of the linear regression model, but instead of using the sum of squares of the regression coefficients, it uses the absolute value of the regression coefficients.\n",
    "\n",
    "The cost function for Lasso regression is defined as:\n",
    "\n",
    "J(β) = RSS(β) + αΣ|βi|\n",
    "\n",
    "where RSS(β) is the residual sum of squares, βi is the ith regression coefficient, and α is the regularization parameter that controls the strength of the regularization. The Lasso regression model aims to minimize this cost function by adjusting the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3033a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0 [0.25 0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "# Let's import required libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Let's Generate some sample data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([4, 5, 6])\n",
    "\n",
    "# Let's Create the Lasso regression model\n",
    "model = Lasso(alpha=0.5)\n",
    "\n",
    "# Let's Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print the model coefficients\n",
    "print(model.intercept_, model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd0fe4",
   "metadata": {},
   "source": [
    "## 6. Elastic Net Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958bb0f",
   "metadata": {},
   "source": [
    "Elastic Net Regression is a linear regression model that combines the L1 and L2 regularization methods to achieve both feature selection and parameter shrinkage. The L1 regularization method, also known as Lasso, shrinks the less important features to zero, while the L2 regularization method, also known as Ridge, shrinks all features toward zero.\n",
    "\n",
    "The elastic net regression model adds a penalty term that is a combination of the L1 and L2 penalty terms. This penalty term is controlled by a hyperparameter alpha, which determines the degree of regularization. When alpha is set to 0, the model is equivalent to the ordinary least squares regression, while as alpha approaches infinity, all coefficients are shrunk towards zero.\n",
    "\n",
    "The elastic net regression model is useful when dealing with high-dimensional datasets where there are many correlated features. It can handle situations where the number of features is larger than the number of observations, and it can select a group of correlated variables together, unlike Lasso, which tends to select one feature from a group of correlated features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2193c17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 147.57\n"
     ]
    }
   ],
   "source": [
    "# Let's import required libraries\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Let's Generate sample dataset\n",
    "X, y = make_regression(n_features=10, random_state=0)\n",
    "\n",
    "# Let's Split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Let's Initialize and fit the model\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=0)\n",
    "enet.fit(X_train, y_train)\n",
    "\n",
    "# Let's Predict on the testing set\n",
    "y_pred = enet.predict(X_test)\n",
    "\n",
    "# Compute mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean squared error: {:.2f}\".format(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87644b8",
   "metadata": {},
   "source": [
    "In this example, we first generate a sample dataset with 10 features and split it into a training and testing set using the train_test_split() function from scikit-learn. We then initialize an Elastic Net Regression model with an alpha value of 0.1 and an L1 ratio of 0.5. The alpha value controls the degree of regularization, and the L1 ratio determines the balance between L1 and L2 regularization.\n",
    "\n",
    "We then fit the model to the training data using the fit() method, and predict on the testing set using the predict() method. Finally, we compute the mean squared error between the predicted and true values using the mean_squared_error() function from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80114de5",
   "metadata": {},
   "source": [
    "## 7. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa2cb9",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical method used to analyze and model the relationship between a binary outcome variable and one or more predictor variables. The goal is to estimate the probability of the binary outcome variable (e.g., yes/no, true/false, success/failure) given the values of the predictor variables.\n",
    "\n",
    "Logistic regression is a type of generalized linear model (GLM) that uses a logistic function (also known as a sigmoid function) to transform a linear combination of predictor variables into a predicted probability. The logistic function has an S-shaped curve that ranges from 0 to 1, and it maps any real-valued input to a probability between 0 and 1.\n",
    "\n",
    "The logistic regression model assumes that the relationship between the predictor variables and the binary outcome variable is linear on the logit scale, which means that the log odds of the binary outcome variable is a linear function of the predictor variables. The log odds is the natural logarithm of the odds ratio, which is the ratio of the probability of the binary outcome variable to the probability of the complementary outcome variable.\n",
    "\n",
    "Logistic regression can be used for both binary and multinomial outcomes, as well as for ordinal outcomes using proportional odds logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3701979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Let's import required libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Let's Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Let's Split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# Let's Initialize and fit the model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Let's Predict on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f7684",
   "metadata": {},
   "source": [
    "## These are some of the most common types of linear regression models, each with its own strengths and weaknesses. The choice of model depends on the specific problem at hand and the data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771e891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
